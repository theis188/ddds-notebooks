{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Context: Latent Dirichlet Allocation\n",
    "\n",
    "### An important language processing technique\n",
    "\n",
    "A big part of machine learning is natural language processing (NLP). NLP is a field dedicated to understanding the structure of language (i.e. semantics) but also document classification and information retrieval (e.g. search & recommendation). LDA belongs mostly to this latter group dealing with document classification.\n",
    "\n",
    "# Background of document classification\n",
    "\n",
    "Document classification is not a new field, and LDA is not the first or only technique used. This section gives a basic introduction to some of the ideas and techniques.\n",
    "\n",
    "### Generative vs. Discriminative\n",
    "\n",
    "One key distinction between classification algorithms is that between generative and descriptive algorithms. Given data $x$, and labels $y$, discriminative algorithms directly calculate the probability of each label $y$ conditioning on the data $x$, in other words the quantity $p(y|x)$. One example would be logistic regression, where the probability of an event is calculated directly as a function of observed data.\n",
    "\n",
    "On the other hand, generative models posit some data process by which data is generated, under each label $y$. This allows for calculation of the probability of observing the data $x$ under each label $y$, in other words, the quantity $p(x|y)$. The LDA algorithm falls into this category.\n",
    "\n",
    "### Bag of Words & Term frequency\n",
    "\n",
    "A document is represented as a bag of words where each word is counted, but order is not important. One basic representation of a 'document' is simply term frequency. For every document, a term frequency $a_i$ is assigned for each word $i$.\n",
    "\n",
    "$TF: \\, a_i = \\dfrac{n_i}{n_{total}} = \\dfrac{ \\textrm {Number of occurences of word i} } {\\textrm{Total number of words in document}}$\n",
    "\n",
    "Very common words (\"the\", \"is\", \"and\", etc.), called stop words, are often ommitted. Each document can now be represented as a vector $\\vec{\\mathbf{a}}$ of the term frequencies $a_i$. This allows for comparisons between documents, for example Euclidean distance between two documents $\\vec{\\mathbf{a_1}}$ and $\\vec{\\mathbf{a_2}}$:\n",
    "\n",
    "$Distance(\\vec{\\mathbf{a_1}},\\vec{\\mathbf{a_2}}) = \\lVert \\vec{\\mathbf{a_1}} - \\vec{\\mathbf{a_2}} \\rVert $\n",
    "\n",
    "### Term frequency - inverse document frequency\n",
    "\n",
    "One add-on to term frequency is term frequency-inverse document frequency, TF-IDF. TF-IDF adds an additional factor to the term frequencies which adds extra weight to terms that occur only rarely.\n",
    "\n",
    "$TFIDF:\\, a_i = \\dfrac{n_i}{n_{total}} log \\left[ \\dfrac{ \\textrm {Total number of documents} } {\\textrm{Number of documents containing word i}}\\right]  $\n",
    "\n",
    "\n",
    "### Latent semantic indexing (LSI)\n",
    "\n",
    "Latent Semantic Indexing (LSI) seeks further dimensionality reductions. To accomplish this, it performs a singular value decomposition (SVD) on the matrix of TF-IDF vectors in the corpus. \n",
    "\n",
    "This procedure seeks to find a lower-dimensional linear combination of some set of column vectors that approximates the true TF-IDF vectors for each document. The weights on this set of columns become the new, lower dimensional description of each document--i.e. topics. This procedure is analagous to principal component analysis.\n",
    "\n",
    "### probabilistic LSI  (pLSI)\n",
    "\n",
    "pLSI is a step from LSI towards LDA. In pLSI, each document is still represented as a linear combination of topics. However, rather than simply represent static vectors of term frequency, the topic weights represent probabilities in a multinomial distribution. For each word, a topic is chosen, weighted by some vector. Then, conditioning on the topic, a word is chosen, conditioning on the topic. \n",
    "\n",
    "This model is partly generative, and has a statistical process for building documents, conditioning on a multinomial distribution of topics. LDA a step further and specifies a process for the generation of this document-level distribution by corpus-level parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Latent Dirichlet Allocation Model\n",
    "\n",
    "Latent Dirichlet Analysis (LDA) is a generative model, meaning it posits a data process for the generation of documents.\n",
    "\n",
    "The model goes as follows:\n",
    "\n",
    "<ol type = \"1\">\n",
    "<li>For each document, select a parameter $\\theta$:</li>\n",
    "    <ol type = \"i\">\n",
    "    <li>This parameter is a vector which specifies the weights given to different 'topics' within the model.</li>\n",
    "    <li>On a technical note, the parameter $\\theta$ is drawn from what's called a Dirichlet distribution. This is a multivalued distribution constrained such that each value is between 0 and 1 and they sum to 1 (forming a so-called 'simplex').</li> \n",
    "    <li>Specifically: $\\theta \\sim Dirichlet(\\alpha)$. $\\alpha$ here is a vector-valued top-level parameter specifying the expected value of each component of $\\theta$.</li> \n",
    "    <li>This step, and the $\\alpha$ parameter differentiate LDA from pLSI.</li>\n",
    "    </ol>\n",
    "\n",
    "<li>For each word, first a topic is chosen, weighted by the parameter $\\theta$, selected in step 1.</li>\n",
    "    <ol Type = \"i\">\n",
    "    <li>Conditioning on the topic, a word is chosen, weighted by the top-level/corpus-level parameter $\\beta$.</li>\n",
    "    <li>Here, $\\beta$ is a top-level matrix-valued parameter. The columns of $\\beta$ represent topics and the rows are words. </li>\n",
    "    <li>The individual values in each column are the probabilities of each word.</li>\n",
    "    </ol>\n",
    "\n",
    "<li>This process is repeated for each word in a document. These steps represent a generative model of how documents in a corpus are constructed. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Model\n",
    "\n",
    "If the technical explanation doesn't quite make sense yet, fear not, many people have an incomplete or incorrect understanding of what LDA actually does. So let's take a closer look at topics, how they work in NLP and their relationship with LDA.\n",
    "\n",
    "### Topics and NLP\n",
    "\n",
    "Topics are essentially a way of categorizing documents. Think of a topic as basically a word cloud of related words that tend to appear together. Documents are then weighted mixtures of topics.\n",
    "\n",
    "<img src=\"topics2.png\"> </img>\n",
    "\n",
    "Why would we want to represent documents as an array of topics? One is dimensionality reduction. Instead of looking at every word in a document . This can help greatly when comparing large numbers of documents, for example in recommender systems and information retrieval (search). A recommender or search engine will choose documents with similar mixtures of topics.\n",
    "\n",
    "### Topics and LDA\n",
    "\n",
    "Many people think of LDA and topics as interchangeable. LDA generates topics, and topics are generated by LDA. In fact, there are many ways to generate topics from a corpus and some have been outlined above in the LSI and pLSI sections. The unique part of LDA is that it constrains topic mixtures to a Dirichlet distribution (i.e. $\\theta \\sim Dir(\\alpha)$). Look here at a cartoon representation of a 3-topic Dirichlet heat map.\n",
    "\n",
    "<img src=\"dirichlet.png\"> </img>\n",
    "\n",
    "The axes represent the weight a topic carries in a document. Since the 3 topic weights must sum to 1, we can represent the three weights in two dimensions. The Dirichlet distribution assigns a likelihood to each combination of topic weights for a document. The shape of the Dirichlet distribution makes it likely that topic weightings will clump together towards one central tendancy. On the other hand, techniques like LSI and pLSI treat any combination of topic weights as equally likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's different about LDA?\n",
    "\n",
    "Rather than approaching the topic-weighting of each document as unrelated, as in pLSI, LDA treats them as coming from a common distribution. This is consistent with the view that authors of documents in a corpus, writing with a common language and coming, mostly, from a common culture, will have some central tendencies when it comes to writing style, content and so on.\n",
    "\n",
    "This insight is an assumption, but to the degree it is true, it will aid inference about topic-level mixtures and document classification. In other words, since LDA presupposes a statistical relationship between documents in a corpus, the performance of LDA will necessarily depend on the nature of the corpus. Specifically, the Dirichlet is a unimodal distribution, which implies that documents in a corpus tend to have similar proportions of topics. In a sparse or highly multimodal corpus, where documents strongly clump together into distinctly different topic mixtures, this may not be a safe representation. On the other hand, in a unified or very large and diverse corpus, the Dirichlet representation of topic mixtures may be better. \n",
    "\n",
    "From the standpoint of classifying a new document in the corpus, the Dirichlet distribution functions in roughly the same way as a regularizing prior does in Bayesian modeling. It more strongly weights topic mixtures that are very plausible, while de-emphasizing ones that are unlikely, under the statistical model.\n",
    "\n",
    "### Computational LDA\n",
    "\n",
    "In the analysis of a corpus, what we really want is an estimate of the various $\\theta$ parameters for document classification, in other words the topic weights. Documents with similar $\\theta$ values will be similar, and this can help with recommendation, information retrieval and so on.\n",
    "\n",
    "The authors of <a href=\"http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\">the LDA paper</a> inform us that there is no analytical solution for the corpus-level parameters $\\alpha$ & $\\beta$ or the document level parameters $\\theta$. However, computational procedures such as a Markov Chain Monte Carlo on the posterior distribution are able to provide usable solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations of LDA\n",
    "\n",
    "LDA is implemented in the sci-kit learn library. See the documentation <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='online', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_jobs=1, n_topics=10, perp_tol=0.1, random_state=None,\n",
      "             topic_word_prior=None, total_samples=1000000.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.decomposition as dc\n",
    "\n",
    "LDA1 = dc.LatentDirichletAllocation()\n",
    "print LDA1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
