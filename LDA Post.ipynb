{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Context: Latent Dirichlet Allocation\n",
    "\n",
    "### An important language processing technique\n",
    "\n",
    "A big part of machine learning is natural language processing (NLP). NLP is a field dedicated to understanding the structure of language (i.e. semantics) but also document classification and information retrieval (e.g. search & recommendation). LDA belongs mostly to this latter group dealing with document classification.\n",
    "\n",
    "### Background of document classification\n",
    "\n",
    "Document classification is not a new field, and LDA is not the first or only technique used. This section gives a basic introduction to some of the ideas and techniques.\n",
    "\n",
    "### Generative vs. Discriminative\n",
    "\n",
    "One $p(x|y\n",
    ")$\n",
    "\n",
    "### Bag of Words & Term frequency\n",
    "\n",
    "A document is represented as a bag of words where each word is counted, but order is not important. One basic representation of a 'document' is simply term frequency. For every document, a term frequency $a_i$ is assigned for each word $i$.\n",
    "\n",
    "$TF: \\, a_i = \\dfrac{n_i}{n_{total}} = \\dfrac{ \\textrm {Number of occurences of word i} } {\\textrm{Total number of words in document}}$\n",
    "\n",
    "Very common words (\"the\", \"is\", \"and\", etc.), called stop words, are often ommitted. Each document can now be represented as a vector $\\vec{\\mathbf{a}}$ of the term frequencies $a_i$. This allows for comparisons between documents, for example Euclidean distance between two documents $\\vec{\\mathbf{a_1}}$ and $\\vec{\\mathbf{a_2}}$:\n",
    "\n",
    "$Distance(D1)$\n",
    "\n",
    "### Term frequency - inverse document frequency\n",
    "\n",
    "One add-on to term frequency is term frequency-inverse document frequency, TF-IDF. TF-IDF adds an additional factor to the term frequencies which adds extra weight to terms that occur only rarely.\n",
    "\n",
    "$TFIDF:\\, a_i = \\dfrac{n_i}{n_{total}} log \\left[ \\dfrac{ \\textrm {Total number of documents} } {\\textrm{Number of documents containing word i}}\\right]  $\n",
    "\n",
    "\n",
    "### Latent semantic indexing (LSI)\n",
    "\n",
    "### probabilistic LSI  (pLSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "display(Math(r'F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$${a_i = \\frac{n_i}{n}}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'{a_i = \\frac{n_i}{n}}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
