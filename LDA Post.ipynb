{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Context: Latent Dirichlet Allocation\n",
    "\n",
    "### An important language processing technique\n",
    "\n",
    "A big part of machine learning is natural language processing (NLP). NLP is a field dedicated to understanding the structure of language (i.e. semantics) but also document classification and information retrieval (e.g. search & recommendation). LDA belongs mostly to this latter group dealing with document classification.\n",
    "\n",
    "# Background of document classification\n",
    "\n",
    "Document classification is not a new field, and LDA is not the first or only technique used. This section gives a basic introduction to some of the ideas and techniques.\n",
    "\n",
    "### Generative vs. Discriminative\n",
    "\n",
    "One key distinction between classification algorithms is that between generative and descriptive algorithms. Given data $x$, and labels $y$, discriminative algorithms directly calculate the probability of each label $y$ conditioning on the data $x$, in other words the quantity $p(y|x)$. One example would be logistic regression, where the probability of an event is calculated directly as a function of observed data.\n",
    "\n",
    "On the other hand, generative models posit some data process by which data is generated, under each label $y$. This allows for calculation of the probability of observing the data $x$ under each label $y$, in other words, the quantity $p(x|y)$. The LDA algorithm falls into this category.\n",
    "\n",
    "### Bag of Words & Term frequency\n",
    "\n",
    "A document is represented as a bag of words where each word is counted, but order is not important. One basic representation of a 'document' is simply term frequency. For every document, a term frequency $a_i$ is assigned for each word $i$.\n",
    "\n",
    "$TF: \\, a_i = \\dfrac{n_i}{n_{total}} = \\dfrac{ \\textrm {Number of occurences of word i} } {\\textrm{Total number of words in document}}$\n",
    "\n",
    "Very common words (\"the\", \"is\", \"and\", etc.), called stop words, are often ommitted. Each document can now be represented as a vector $\\vec{\\mathbf{a}}$ of the term frequencies $a_i$. This allows for comparisons between documents, for example Euclidean distance between two documents $\\vec{\\mathbf{a_1}}$ and $\\vec{\\mathbf{a_2}}$:\n",
    "\n",
    "$Distance(\\vec{\\mathbf{a_1}},\\vec{\\mathbf{a_2}}) = \\lVert \\vec{\\mathbf{a_1}} - \\vec{\\mathbf{a_2}} \\rVert $\n",
    "\n",
    "### Term frequency - inverse document frequency\n",
    "\n",
    "One add-on to term frequency is term frequency-inverse document frequency, TF-IDF. TF-IDF adds an additional factor to the term frequencies which adds extra weight to terms that occur only rarely.\n",
    "\n",
    "$TFIDF:\\, a_i = \\dfrac{n_i}{n_{total}} log \\left[ \\dfrac{ \\textrm {Total number of documents} } {\\textrm{Number of documents containing word i}}\\right]  $\n",
    "\n",
    "\n",
    "### Latent semantic indexing (LSI)\n",
    "\n",
    "Latent Semantic Indexing (LSI) seeks further dimensionality reductions. To accomplish this, it performs a singular value decomposition (SVD) on the matrix of TF-IDF vectors in the corpus. \n",
    "\n",
    "This procedure seeks to find a lower-dimensional linear combination of some set of column vectors that approximates the true TF-IDF vectors for each document. The weights on this set of columns become the new, lower dimensional description of each document--i.e. topics. This procedure is analagous to principal component analysis.\n",
    "\n",
    "### probabilistic LSI  (pLSI)\n",
    "\n",
    "pLSI is a step from LSI towards LDA. In pLSI, each document is still represented as a linear combination of topics. However, rather than simply represent static vectors of term frequency, the topic weights represent probabilities in a multinomial distribution. For each word, a topic is chosen, weighted by some vector. Then, conditioning on the topic, a word is chosen, conditioning on the topic. \n",
    "\n",
    "This model is partly generative, and has a statistical process for building documents, conditioning on a multinomial distribution of topics. LDA a step further and specifies a process for the generation of this document-level distribution by corpus-level parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Latent Dirichlet Allocation Model\n",
    "\n",
    "Latent Dirichlet Analysis (LDA) is a generative model, meaning it posits a data process for the generation of documents.\n",
    "\n",
    "The model goes as follows:\n",
    "\n",
    "<ol type = \"1\">\n",
    "<li>For each document, select a parameter $\\theta$:</li>\n",
    "    <ol type = \"i\">\n",
    "    <li>This parameter is a vector which specifies the weights given to different 'topics' within the model.</li>\n",
    "    <li>The parameter $\\theta$ is drawn from what's called a Dirichlet distribution. This is a multivalued distribution constrained such that each value is between 0 and 1 and they sum to 1 (forming a so-called 'simplex').</li> \n",
    "    <li>Specifically: $\\theta \\sim Dirichlet(\\alpha)$. $\\alpha$ here is a vector-valued top-level parameter specifying the expected value of each component of $\\theta$.</li> \n",
    "    <li>The Dirichlet distribution has the following probability density function:</li>\n",
    "    <li>$p(x)\\sim \\prod_{i=1}^{K}{x_i^{\\alpha_i-1}} $ normalized by a constant. $K$ is the number of dimensions supported by the distribution, which would equal the number of topics in your LDA model. If this is confusing, hold on. We'll address the Dirichlet distribution more in a bit.</li>\n",
    "    <li>The parameter values $\\alpha_i$ must be > 0.\n",
    "    <li>This step, and the $\\alpha$ parameter differentiate LDA from pLSI.</li>\n",
    "    </ol>\n",
    "\n",
    "<li>For each word, first a topic is chosen, weighted by the parameter $\\theta$, selected in step 1.</li>\n",
    "    <ol Type = \"i\">\n",
    "    <li>Conditioning on the topic, a word is chosen, weighted by the top-level/corpus-level parameter $\\beta$.</li>\n",
    "    <li>Here, $\\beta$ is a top-level matrix-valued parameter. The columns of $\\beta$ represent topics and the rows are words. </li>\n",
    "    <li>The individual values in each column are the probabilities of each word.</li>\n",
    "    </ol>\n",
    "\n",
    "<li>This process is repeated for each word in a document. These steps represent a generative model of how documents in a corpus are constructed. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dirichlet Distribution\n",
    "\n",
    "Understanding the Dirichlet distribution is key to understanding LDA. As mentioned above, it has the following density function:\n",
    "\n",
    "$p(x)\\sim \\prod_{i=1}^{K}{x_i^{\\alpha_i-1}} $\n",
    "\n",
    "I'm omitting a normalization factor for clarity. The components of $\\alpha$ must all be greater than zero, but are typically greater than one, and you'll see why in a little bit.\n",
    "\n",
    "The values of x must sum to 1, in other words: $\\sum_{i=1}^{K}{x_i}=1$. This means that a 3 dimensional Dirichlet distribution can be represented on a 2-dimensional plot, since $x_3 = 1-x_1-x_2$. This type of plot is known as a ternary plot and is often used to show composition in a 3-component mixture.\n",
    "\n",
    "The mean value of component i, $E[x_i]$, in the Dirichlet distribution is the fractional \"weight\" of the corresponding parameter component, defined as follows: $E[x_i] = \\dfrac{\\alpha_i}{\\sum_{j=1}^{K}{\\alpha_j}}$\n",
    "\n",
    "Let's see what happens when we have the Dirichlet parameters $\\alpha_1=\\alpha_2=\\alpha_3=1.2$\n",
    "\n",
    "<img src = \"Dir_12_12_12i.png\"></img>\n",
    "\n",
    "We can see that the distribution is nearly uniform over most of its range, except when one of the $x_i$ gets close to zero. This equates to a nearly uniform distribution of topics in documents over a corpus. The closer the parameters get to 1, the closer the distribution gets to uniform. When the parameters are greater then 1, as here, the distribution is bubble shaped, or concave down. When the parameters are less than 1, the distribution becomes bowl-shaped, with the distribution weight focused at the corners.\n",
    "\n",
    "Now let's set $\\alpha_1=\\alpha_2=\\alpha_3=2$\n",
    "\n",
    "<img src = \"Dir_2_2_2.png\"></img>\n",
    "\n",
    "We can see the distribution is much more focused in the center now. As the parameters get larger, the distribution gets pointier.\n",
    "\n",
    "Now let's try $\\alpha_1=\\alpha_2=2;\\alpha_3=5$\n",
    "\n",
    "<img src = \"Dir_2_2_5.png\"></img>\n",
    "\n",
    "We can now see that this distribution is focused near where $x_3 = 1$. This is because the $\\alpha_3$ parameter carries most of the weight. This would represent a corpus where topic 3 is heavily represented in most of the documents.\n",
    "\n",
    "R code for these plots is available in this folder at <a href=\"Dirichlet.r\">Dirichlet.r</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Model\n",
    "\n",
    "If the technical explanation doesn't quite make sense yet, fear not, many people have an incomplete or incorrect understanding of what LDA actually does. So let's take a closer look at topics, how they work in NLP and their relationship with LDA.\n",
    "\n",
    "### Topics and NLP\n",
    "\n",
    "Topics are essentially a way of categorizing documents. Think of a topic as basically a word cloud of related words that tend to appear together. Documents are then weighted mixtures of topics.\n",
    "\n",
    "<img src=\"topics2.png\"> </img>\n",
    "\n",
    "Why would we want to represent documents as an array of topics? One is dimensionality reduction. Instead of looking at every word in a document . This can help greatly when comparing large numbers of documents, for example in recommender systems and information retrieval (search). A recommender or search engine will choose documents with similar mixtures of topics.\n",
    "\n",
    "### Topics and LDA\n",
    "\n",
    "Many people think of LDA and topics as interchangeable. LDA generates topics, and topics are generated by LDA. In fact, there are many ways to generate topics from a corpus and some have been outlined above in the LSI and pLSI sections. The unique part of LDA is that it constrains topic mixtures to a Dirichlet distribution (i.e. $\\theta \\sim Dir(\\alpha)$). Look here at a cartoon representation of a 3-topic Dirichlet heat map.\n",
    "\n",
    "<img src=\"dirichlet.png\"> </img>\n",
    "\n",
    "The axes represent the weight a topic carries in a document. Since the 3 topic weights must sum to 1, we can represent the three weights in two dimensions. The Dirichlet distribution assigns a likelihood to each combination of topic weights for a document. The shape of the Dirichlet distribution makes it likely that topic weightings will clump together towards one central tendancy. On the other hand, techniques like LSI and pLSI treat any combination of topic weights as equally likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's different about LDA?\n",
    "\n",
    "Rather than approaching the topic-weighting of each document as unrelated, as in pLSI, LDA treats them as coming from a common distribution. This is consistent with the view that authors of documents in a corpus, writing with a common language and coming, mostly, from a common culture, will have some central tendencies when it comes to writing style, content and so on.\n",
    "\n",
    "This insight is an assumption, but to the degree it is true, it will aid inference about topic-level mixtures and document classification. In other words, since LDA presupposes a statistical relationship between documents in a corpus, the performance of LDA will necessarily depend on the nature of the corpus. Specifically, the Dirichlet is a unimodal distribution, which implies that documents in a corpus tend to have similar proportions of topics. In a sparse or highly multimodal corpus, where documents strongly clump together into distinctly different topic mixtures, this may not be a safe representation. On the other hand, in a unified or very large and diverse corpus, the Dirichlet representation of topic mixtures may be better. \n",
    "\n",
    "From the standpoint of classifying a new document in the corpus, the Dirichlet distribution functions in roughly the same way as a regularizing prior does in Bayesian modeling. It more strongly weights topic mixtures that are very plausible, while de-emphasizing ones that are unlikely, under the statistical model.\n",
    "\n",
    "# Computational LDA\n",
    "\n",
    "In the analysis of a corpus, one thing we might want is an estimate of the various $\\theta$ parameters for document classification, in other words the topic weights. Documents with similar $\\theta$ values will be similar, and this can help with recommendation, information retrieval and so on.\n",
    "\n",
    "The authors of <a href=\"http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\">the LDA paper</a> inform us that there is no analytical solution for the corpus-level parameters $\\alpha$ & $\\beta$ or the document level parameters $\\theta$. However, computational procedures such as a Markov Chain Monte Carlo on the posterior distribution are able to provide usable solutions.\n",
    "\n",
    "### Author's preferred method\n",
    "\n",
    "The authors present a method called variational inference, based on the idea of minimizing the KL-divergence between the true solution and intermediate solutions. They present a relatively simple iterative algorithm for calculating the most likely $\\theta$ and $\\alpha$ values <i>for each document</i>, and then show how to use these to iteratively estimate the corpus level parameters $\\alpha$ and $\\beta$. See <a href=\"http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\">their paper</a>, section 5, and appendices A.3 and A.4.\n",
    "\n",
    "### Markov Chain Monte Carlo\n",
    "\n",
    "Due to the very high number of parpameters, a Markov Chain Monte Carlo method is likely to be far too slow. However, this is one process that could be used to attempt it.\n",
    "\n",
    "To calculate the likelihood, first pick random values for every parameter value, $\\alpha$, $\\beta$ and $\\theta$.\n",
    "\n",
    "The joint likelihood of the $\\theta$ parameters would be:\n",
    "\n",
    "$\\prod_{i=1}^{n}{p(\\theta_i|\\alpha)}$ where $n$ is the number of documents, $\\theta_i$ are the vector-valued document level topic mixture parameters and the $p(\\theta|\\alpha)$ is the density function of the Dirichlet distribution, parametrized on $\\alpha$.\n",
    "\n",
    "The probability of any individual word of row index j in $\\beta$ matrix, in document i would be $\\beta_{(j)}\\theta_i$, where $\\beta_{(j)}$ is the jth row of $\\beta$ and $\\theta_i$ is the document level topic mixture drawn from the Dirichlet distribution. The joint likelihood of the all words given $\\alpha$, $\\beta$ and $\\theta$ would be the product of the over every word in the corpus.\n",
    "\n",
    "The overall likelihood of the model would be the product of the 'theta' likelihood and the 'word' likelihood. This could be used in the Metropolis algorithm or in a simple maximum likelihood estimate search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations of LDA\n",
    "\n",
    "LDA is implemented in the sci-kit learn library. See the documentation <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='online', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_jobs=1, n_topics=10, perp_tol=0.1, random_state=None,\n",
      "             topic_word_prior=None, total_samples=1000000.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.decomposition as dc\n",
    "\n",
    "LDA1 = dc.LatentDirichletAllocation()\n",
    "print LDA1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
